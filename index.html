<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Ruby WASM Sound Visualizer</title>
  <style>
    body {
      margin: 0;
      overflow: hidden;
      background: #000;
      font-family: Arial, sans-serif;
    }
    canvas {
      display: block;
    }
    #loading {
      position: fixed;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      color: white;
      font-family: monospace;
      font-size: 20px;
      text-align: center;
      z-index: 100;
    }
    #status {
      position: fixed;
      bottom: 20px;
      left: 20px;
      color: #0cc;
      font-family: monospace;
      font-size: 10px;
      z-index: 50;
    }
    #vrmUpload {
      margin-top: 30px;
    }
    .vrm-upload-btn {
      display: inline-block;
      padding: 12px 24px;
      background: #222;
      color: #0f0;
      border: 1px solid #0f0;
      border-radius: 4px;
      cursor: pointer;
      font-family: monospace;
      font-size: 14px;
      transition: background 0.3s, color 0.3s;
    }
    .vrm-upload-btn:hover {
      background: #0f0;
      color: #000;
    }
    #vrmFileName {
      margin-top: 10px;
      font-size: 12px;
      color: #888;
    }
    /* VJ Pad prompt overlay */
    #vjPrompt {
      display: none;
      position: fixed;
      bottom: 55px;
      left: 20px;
      right: 20px;
      z-index: 60;
      font-family: monospace;
      font-size: 12px;
    }
    #vjPrompt.active { display: flex; align-items: center; gap: 6px; }
    #vjPromptInput {
      flex: 1;
      background: rgba(0, 0, 0, 0.25);
      color: #0f0;
      border: none;
      border-radius: 3px;
      padding: 4px 8px;
      font-family: monospace;
      font-size: 12px;
      outline: none;
    }
    #vjPromptInput::placeholder { color: #060; }
    #vjPromptResult {
      color: #0f0;
      white-space: nowrap;
      max-width: 40%;
      overflow: hidden;
      text-overflow: ellipsis;
    }
    #vjPromptResult.error { color: #f44; }
  </style>
</head>
<body>
  <!-- Tab video background (hidden until tab capture starts) -->
  <video id="tabVideo" autoplay playsinline muted
         style="display:none; position:fixed; top:0; left:0; width:100%; height:100%; object-fit:cover; z-index:-2;"></video>
  <div id="tabOverlay"
       style="display:none; position:fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.5); z-index:-1;"></div>

  <div id="loading">
    <div id="loadingTitle">Loading Ruby WASM Sound Visualizer...</div>
    <div id="loadingStatus" style="font-size: 12px; margin-top: 20px;"></div>
    <div id="vrmUpload" style="display: none;">
      <label for="vrmFileInput" class="vrm-upload-btn">Upload VRM File</label>
      <input type="file" id="vrmFileInput" accept=".vrm" style="display: none;">
      <button id="skipVrmBtn" class="vrm-upload-btn" style="margin-left: 10px;">Start without VRM</button>
      <button id="captureTabBtn" class="vrm-upload-btn" style="margin-left: 10px;">Capture Tab</button>
      <div id="vrmFileName"></div>
    </div>
  </div>
  <div id="vjPrompt">
    <span style="color: #0f0;">&gt;</span>
    <input id="vjPromptInput" type="text" placeholder="c 1; s 2.0; bm 4.0" autocomplete="off" spellcheck="false">
    <span id="vjPromptResult"></span>
  </div>
  <div id="status">
    <div style="line-height: 1.0;"><span id="fpsCounter">FPS: 0</span>  |  <span id="paramInfo"></span></div>
    <div id="debugInfo" style="margin-top: 1px; line-height: 1.0; white-space: nowrap; overflow-x: auto;"></div>
    <div id="keyGuide" style="margin-top: 1px; line-height: 1.0; color: #888; font-size: 9px; white-space: pre-wrap;"></div>
  </div>

  <!-- Ruby WASM (must load before async module scripts) -->
  <script src="https://cdn.jsdelivr.net/npm/@ruby/4.0-wasm-wasi@2.8.1/dist/browser.script.iife.js"></script>

  <!-- Import Maps で Three.js 依存関係を解決 -->
  <script type="importmap">
{
  "imports": {
    "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
    "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/",
    "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3/lib/three-vrm.module.min.js"
  }
}
  </script>

  <!-- Three.js とポストプロセッシングをグローバルに登録 -->
  <script type="module">
    import * as THREE from 'three';
    import { EffectComposer } from 'three/addons/postprocessing/EffectComposer.js';
    import { RenderPass } from 'three/addons/postprocessing/RenderPass.js';
    import { UnrealBloomPass } from 'three/addons/postprocessing/UnrealBloomPass.js';
    import { OutputPass } from 'three/addons/postprocessing/OutputPass.js';
    import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
    import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';

    // Ruby から使えるようにグローバルに登録
    window.THREE = THREE;
    window.EffectComposer = EffectComposer;
    window.RenderPass = RenderPass;
    window.UnrealBloomPass = UnrealBloomPass;
    window.OutputPass = OutputPass;
    window.GLTFLoader = GLTFLoader;
    window.VRMLoaderPlugin = VRMLoaderPlugin;
    window.VRMUtils = VRMUtils;
    window.THREE_READY = true;

    console.log('[Three.js] r' + THREE.REVISION + ' loaded with postprocessing + VRM support');
  </script>

  <!-- Ruby Code Blocks (external files) -->
  <script type="text/ruby" src="src/ruby/visualizer_policy.rb"></script>
  <script type="text/ruby" src="src/ruby/math_helper.rb"></script>
  <script type="text/ruby" src="src/ruby/js_bridge.rb"></script>
  <script type="text/ruby" src="src/ruby/frequency_mapper.rb"></script>
  <script type="text/ruby" src="src/ruby/audio_analyzer.rb"></script>
  <script type="text/ruby" src="src/ruby/color_palette.rb"></script>
  <script type="text/ruby" src="src/ruby/particle_system.rb"></script>
  <script type="text/ruby" src="src/ruby/geometry_morpher.rb"></script>
  <script type="text/ruby" src="src/ruby/camera_controller.rb"></script>
  <script type="text/ruby" src="src/ruby/bloom_controller.rb"></script>
  <script type="text/ruby" src="src/ruby/effect_manager.rb"></script>
  <script type="text/ruby" src="src/ruby/keyboard_handler.rb"></script>
  <script type="text/ruby" src="src/ruby/vj_pad.rb"></script>
  <script type="text/ruby" src="src/ruby/debug_formatter.rb"></script>
  <script type="text/ruby" src="src/ruby/bpm_estimator.rb"></script>
  <script type="text/ruby" src="src/ruby/frame_counter.rb"></script>
  <script type="text/ruby" src="src/ruby/vrm_dancer.rb"></script>
  <script type="text/ruby" src="src/ruby/vrm_material_controller.rb"></script>
  <script type="text/ruby" src="src/ruby/main.rb"></script>

  <script>
    // === Log Buffer for Chrome MCP Access ===
    // Structured ring buffer accessible from DevTools: window.logBuffer.getLast(20)
    window.logBuffer = {
      entries: [],
      maxSize: 500,
      add(level, source, message) {
        this.entries.push({
          ts: new Date().toISOString().substr(11, 12),
          level, source, message
        });
        if (this.entries.length > this.maxSize) {
          this.entries = this.entries.slice(-this.maxSize);
        }
      },
      getLast(n) { return this.entries.slice(-(n || 20)); },
      getErrors() { return this.entries.filter(e => e.level === 'error'); },
      getRuby() { return this.entries.filter(e => e.source === 'ruby'); },
      getJS() { return this.entries.filter(e => e.source === 'js'); },
      clear() { this.entries = []; },
      dump() {
        return this.entries.map(e =>
          `${e.ts} [${e.level}][${e.source}] ${e.message}`
        ).join('\n');
      }
    };

    // Intercept console methods to capture all output into logBuffer
    ['log', 'warn', 'error'].forEach(method => {
      const original = console[method].bind(console);
      console[method] = function(...args) {
        original(...args);
        try {
          const msg = args.map(a => typeof a === 'object' ? JSON.stringify(a) : String(a)).join(' ');
          const source = msg.startsWith('[Ruby]') ? 'ruby' :
                         msg.startsWith('[JS]') ? 'js' :
                         msg.startsWith('[DEBUG') ? 'debug' : 'other';
          window.logBuffer.add(method === 'log' ? 'info' : method, source, msg);
        } catch (e) { /* ignore logging errors */ }
      };
    });

    // Global state for JavaScript bridge
    let audioContext, analyser, dataArray;
    let scene, camera, renderer, composer;
    let particleSystem, geometryMesh;
    let renderPass, bloomPass;
    let frameCount = 0;  // Used for DOM update throttle only

    // Audio input state
    let micGain = null;
    let micMuted = false;
    let tabStream = null;
    let tabAudioSource = null;
    let tabAudioGain = null;

    // VRM state
    let currentVRM = null;
    let animLastTime = Date.now();

    // Camera rotation control (spherical coordinates)
    let cameraTheta = 0;      // Y-axis rotation (horizontal)
    let cameraPhi = 0;        // X-axis rotation (vertical)
    let cameraRadius = 5;     // Distance from target
    let cameraTarget = null;  // Look-at target (initialized in initThree)

    // Selective Bloom (Layer 1 = bloom objects, Layer 0 = non-bloom objects)
    const BLOOM_LAYER = 1;
    let bloomLayer = null;      // Initialize after THREE.js loads
    let darkMaterial = null;    // Initialize after THREE.js loads
    const materials = {};       // Store original materials during bloom pass

    // VRM bone order (must match Ruby VRMDancer::BONE_ORDER)
    const VRM_BONE_ORDER = [
      'hips', 'spine', 'chest', 'head',
      'leftUpperArm', 'leftLowerArm', 'leftHand',
      'rightUpperArm', 'rightLowerArm', 'rightHand',
      'leftUpperLeg', 'leftLowerLeg',
      'rightUpperLeg', 'rightLowerLeg'
    ];

    // Ruby VM is set by browser.script.iife.js as window.rubyVM
    const getRubyVM = () => window.rubyVM;

    // Web Audio API Setup
    async function initAudio() {
      console.log('[JS] Initializing Web Audio API...');
      try {
        // Reuse AudioContext if already created (e.g. by tab capture during startup)
        if (!audioContext) {
          audioContext = new AudioContext();
        }

        // Chrome のオートプレイポリシー対策: suspended なら resume する
        if (audioContext.state === 'suspended') {
          console.log('[JS] AudioContext is suspended, attempting resume...');
          await audioContext.resume();
          console.log('[JS] AudioContext resumed, state:', audioContext.state);
        }

        // Reuse analyser if already created
        if (!analyser) {
          analyser = audioContext.createAnalyser();
          analyser.fftSize = 2048;
          analyser.smoothingTimeConstant = 0.5;  // ビート検出のため低めに設定
        }
        const bufferLength = analyser.frequencyBinCount;
        dataArray = new Uint8Array(bufferLength);

        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const source = audioContext.createMediaStreamSource(stream);
        micGain = audioContext.createGain();
        micGain.gain.value = 1.0;
        source.connect(micGain);
        micGain.connect(analyser);
        console.log('[JS] Web Audio API initialized successfully, context state:', audioContext.state);

        // getUserMedia 後にまだ suspended なら再度 resume
        if (audioContext.state === 'suspended') {
          console.log('[JS] AudioContext still suspended after getUserMedia, retrying resume...');
          await audioContext.resume();
          console.log('[JS] AudioContext state after retry:', audioContext.state);
        }
      } catch (error) {
        console.error('[JS] Audio init error:', error);
        throw error;
      }
    }

    // Toggle microphone mute/unmute
    function toggleMic() {
      if (!micGain) return;
      micMuted = !micMuted;
      micGain.gain.value = micMuted ? 0 : 1;
      console.log('[JS] Mic ' + (micMuted ? 'muted' : 'unmuted'));
      if (window.rubyOnMicToggle) window.rubyOnMicToggle(micMuted);
    }

    // Set mic mute state explicitly (0=mute, 1=unmute)
    function setMicMute(mute) {
      if (!micGain) return;
      micMuted = !!mute;
      micGain.gain.value = micMuted ? 0 : 1;
      console.log('[JS] Mic ' + (micMuted ? 'muted' : 'unmuted'));
      if (window.rubyOnMicToggle) window.rubyOnMicToggle(micMuted);
    }

    // Start capturing audio/video from another Chrome tab
    async function startTabCapture() {
      try {
        tabStream = await navigator.mediaDevices.getDisplayMedia({
          video: true,
          audio: true
        });

        // Audio: connect tab audio to analyser via gain node
        const audioTracks = tabStream.getAudioTracks();
        if (audioTracks.length > 0) {
          tabAudioSource = audioContext.createMediaStreamSource(
            new MediaStream(audioTracks)
          );
          tabAudioGain = audioContext.createGain();
          tabAudioGain.gain.value = 1.0;
          tabAudioSource.connect(tabAudioGain);
          tabAudioGain.connect(analyser);
          console.log('[JS] Tab audio connected to analyser');
        }

        // Video: display in background video element
        const videoTracks = tabStream.getVideoTracks();
        if (videoTracks.length > 0) {
          const videoEl = document.getElementById('tabVideo');
          videoEl.srcObject = new MediaStream(videoTracks);
          videoEl.play();
          videoEl.style.display = 'block';
          document.getElementById('tabOverlay').style.display = 'block';
          if (renderer && renderer.domElement) {
            renderer.domElement.style.mixBlendMode = 'screen';
          }
          console.log('[JS] Tab video overlay active');
        }

        // Handle stream end (user stops sharing via browser UI)
        tabStream.getTracks().forEach(track => {
          track.addEventListener('ended', stopTabCapture);
        });

        if (window.rubyOnTabToggle) window.rubyOnTabToggle(true);
        console.log('[JS] Tab capture started');
      } catch (error) {
        console.error('[JS] Tab capture error:', error);
        tabStream = null;
      }
    }

    // Stop tab capture and clean up
    function stopTabCapture() {
      if (tabStream) {
        tabStream.getTracks().forEach(t => t.stop());
        tabStream = null;
      }
      if (tabAudioSource) {
        tabAudioSource.disconnect();
        tabAudioSource = null;
      }
      if (tabAudioGain) {
        tabAudioGain.disconnect();
        tabAudioGain = null;
      }
      const videoEl = document.getElementById('tabVideo');
      if (videoEl) {
        videoEl.srcObject = null;
        videoEl.style.display = 'none';
      }
      const overlayEl = document.getElementById('tabOverlay');
      if (overlayEl) overlayEl.style.display = 'none';
      if (renderer && renderer.domElement) {
        renderer.domElement.style.mixBlendMode = 'normal';
      }
      if (window.rubyOnTabToggle) window.rubyOnTabToggle(false);
      console.log('[JS] Tab capture stopped');
    }

    // Toggle tab capture on/off
    function toggleTabCapture() {
      if (tabStream) {
        stopTabCapture();
      } else {
        startTabCapture();
      }
    }

    // Three.js Setup
    function initThree() {
      // Three.js モジュールが読み込まれるまで待機
      if (!window.THREE_READY) {
        console.log('[JS] Waiting for Three.js to load...');
        setTimeout(initThree, 100);
        return;
      }

      console.log('[JS] Initializing Three.js...');

      scene = new THREE.Scene();
      camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
      // Initialize camera target and position with spherical coordinates
      cameraTarget = new THREE.Vector3(0, 0, 0);
      updateCameraPosition();
      // Enable all layers for camera (Layer 0 for VRM, Layer 1 for particles/torus)
      camera.layers.enableAll();

      renderer = new THREE.WebGLRenderer({ antialias: true, alpha: false });
      renderer.setSize(window.innerWidth, window.innerHeight);
      renderer.setClearColor(0x000000);
      document.body.appendChild(renderer.domElement);

      // Post-processing setup
      composer = new EffectComposer(renderer);

      renderPass = new RenderPass(scene, camera);
      composer.addPass(renderPass);

      bloomPass = new UnrealBloomPass(
        new THREE.Vector2(window.innerWidth, window.innerHeight),
        1.5,   // Bloom strength
        0.4,   // Bloom radius
        0.0    // Threshold 0.0 = 全体を輝かせる
      );
      composer.addPass(bloomPass);

      // OutputPass を追加（最終出力用）
      const outputPass = new OutputPass();
      composer.addPass(outputPass);

      // Particle system
      const particleCount = 3000;
      const geometry = new THREE.BufferGeometry();
      const positions = new Float32Array(particleCount * 3);
      const colors = new Float32Array(particleCount * 3);

      for (let i = 0; i < particleCount; i++) {
        positions[i * 3] = (Math.random() - 0.5) * 10;
        positions[i * 3 + 1] = (Math.random() - 0.5) * 10;
        positions[i * 3 + 2] = (Math.random() - 0.5) * 10;
        colors[i * 3] = 0.3;      // dim gray (グレースケール起動)
        colors[i * 3 + 1] = 0.3;
        colors[i * 3 + 2] = 0.3;
      }

      geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));
      geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));

      const material = new THREE.PointsMaterial({
        size: 0.05,
        vertexColors: true,
        transparent: true,
        opacity: 0.8,
        blending: THREE.AdditiveBlending
      });

      particleSystem = new THREE.Points(geometry, material);
      // particleSystem.layers.set(BLOOM_LAYER);  // DISABLED: Enable bloom for particles
      scene.add(particleSystem);

      // Morphing geometry (torus) - ポリゴン削減で FPS 改善（32,64 → 24,32）
      const torusGeometry = new THREE.TorusGeometry(1, 0.4, 12, 16);
      const torusMaterial = new THREE.MeshStandardMaterial({
        color: 0x4d4d4d,  // dim gray (グレースケール起動)
        wireframe: true,
        transparent: true,
        opacity: 0.6,
        emissive: 0xffffff,  // white for uniform glow
        emissiveIntensity: 0.3,  // initial intensity
        metalness: 0.0,
        roughness: 1.0
      });
      geometryMesh = new THREE.Mesh(torusGeometry, torusMaterial);
      // geometryMesh.layers.set(BLOOM_LAYER);  // DISABLED: Enable bloom for torus
      scene.add(geometryMesh);

      // Initialize Selective Bloom materials
      bloomLayer = new THREE.Layers();
      bloomLayer.set(BLOOM_LAYER);
      darkMaterial = new THREE.MeshBasicMaterial({ color: 'black' });

      console.log('[JS] Three.js initialized successfully');

      // Setup keyboard controls for camera rotation
      setupCameraControls();
    }

    // Update camera position using spherical coordinates
    function updateCameraPosition() {
      const x = cameraRadius * Math.cos(cameraPhi) * Math.sin(cameraTheta);
      const y = cameraRadius * Math.sin(cameraPhi);
      const z = cameraRadius * Math.cos(cameraPhi) * Math.cos(cameraTheta);
      camera.position.set(cameraTarget.x + x, cameraTarget.y + y, cameraTarget.z + z);
      camera.lookAt(cameraTarget);
    }

    // Setup keyboard controls for camera rotation
    function setupCameraControls() {
      document.addEventListener('keydown', (event) => {
        const rotationStep = Math.PI / 18;  // 10 degrees

        switch(event.key.toLowerCase()) {
          case 'd':  // Rotate left
            cameraTheta -= rotationStep;
            updateCameraPosition();
            break;
          case 'f':  // Rotate right
            cameraTheta += rotationStep;
            updateCameraPosition();
            break;
          case 'e':  // Rotate up
            cameraPhi = Math.min(cameraPhi + rotationStep, Math.PI / 2 - 0.01);  // Clamp to avoid gimbal lock
            updateCameraPosition();
            break;
          case 'c':  // Rotate down
            cameraPhi = Math.max(cameraPhi - rotationStep, -Math.PI / 2 + 0.01);  // Clamp to avoid gimbal lock
            updateCameraPosition();
            break;
        }
      });

      console.log('[JS] Camera controls initialized (d/f: rotate left/right, e/c: rotate up/down)');
    }

    // VRM file upload promise (or skip)
    function waitForVRMFile() {
      return new Promise((resolve) => {
        const input = document.getElementById('vrmFileInput');
        const skipBtn = document.getElementById('skipVrmBtn');
        const captureBtn = document.getElementById('captureTabBtn');

        input.addEventListener('change', (event) => {
          const file = event.target.files[0];
          if (file) {
            document.getElementById('vrmFileName').textContent = file.name;
            resolve(file);
          }
        });

        skipBtn.addEventListener('click', () => {
          resolve(null);  // No VRM file, start without it
        });

        // Tab capture can be started during startup (independent of VRM choice)
        captureBtn.addEventListener('click', async () => {
          try {
            // Need AudioContext before tab capture for audio routing
            if (!audioContext) {
              audioContext = new AudioContext();
              analyser = audioContext.createAnalyser();
              analyser.fftSize = 2048;
              analyser.smoothingTimeConstant = 0.5;
              dataArray = new Uint8Array(analyser.frequencyBinCount);
            }
            await startTabCapture();
            captureBtn.textContent = 'Tab Captured!';
            captureBtn.disabled = true;
          } catch (error) {
            console.error('[JS] Tab capture during startup error:', error);
          }
        });
      });
    }

    // Load VRM from File object
    async function loadVRMFile(file) {
      return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onload = (event) => {
          const arrayBuffer = event.target.result;
          const loader = new GLTFLoader();
          loader.register((parser) => new VRMLoaderPlugin(parser));

          loader.parse(arrayBuffer, '', (gltf) => {
            currentVRM = gltf.userData.vrm;
            window.currentVRM = currentVRM;  // Expose to global scope for Ruby access

            VRMUtils.removeUnnecessaryVertices(gltf.scene);
            VRMUtils.removeUnnecessaryJoints(gltf.scene);

            // Position the VRM model at center, slightly lower, and closer to camera
            currentVRM.scene.position.set(0, -1.2, 0.3);

            // Scale up the model for better visibility (2x)
            currentVRM.scene.scale.set(2.0, 2.0, 2.0);

            scene.add(currentVRM.scene);

            // Add lights for VRM materials (does not affect unlit particles/torus)
            const dirLight = new THREE.DirectionalLight(0xffffff, 1.0);
            dirLight.position.set(1, 2, 1);
            scene.add(dirLight);
            const ambLight = new THREE.AmbientLight(0x666666);
            scene.add(ambLight);

            // Move torus behind VRM model (background)
            if (geometryMesh) {
              geometryMesh.position.z = -2.0;
            }

            // Initialize VRM materials with emissive for bloom
            // (Ruby will control the intensity dynamically)
            // Use material's base color for emissive to make all parts glow naturally
            // VRM材質はデフォルトのまま（ローダーの設定を尊重）
            // updateVRMMaterial で emissiveIntensity のみ更新する

            console.log('[JS] VRM model loaded successfully');
            resolve();
          }, (error) => {
            console.error('[JS] VRM load error:', error);
            reject(error);
          });
        };
        reader.onerror = (err) => reject(err);
        reader.readAsArrayBuffer(file);
      });
    }

    // Update VRM bones from Ruby dance data
    window.updateVRM = function(rotations, hipsY, blink, mouthV, mouthH) {
      if (!currentVRM) return;

      const humanoid = currentVRM.humanoid;
      const rotArray = Array.from(rotations || []);
      const hipsPositionY = Number(hipsY) || 0;
      const blinkValue = Number(blink) || 0;
      const mouthOpenVertical = Number(mouthV) || 0;
      const mouthOpenHorizontal = Number(mouthH) || 0;

      for (let i = 0; i < VRM_BONE_ORDER.length; i++) {
        const boneName = VRM_BONE_ORDER[i];
        const bone = humanoid.getNormalizedBoneNode(boneName);
        if (bone) {
          const rx = Number(rotArray[i * 3]) || 0;
          const ry = Number(rotArray[i * 3 + 1]) || 0;
          const rz = Number(rotArray[i * 3 + 2]) || 0;
          bone.rotation.set(rx, ry, rz);

          // Hips position (bounce) - use set, not accumulate
          if (boneName === 'hips') {
            bone.position.y = hipsPositionY;
          }
        }
      }

      // Update expressions
      const expressionManager = currentVRM.expressionManager;
      if (expressionManager) {
        // Blink
        expressionManager.setValue('blink', blinkValue);
        expressionManager.setValue('blinkLeft', blinkValue);
        expressionManager.setValue('blinkRight', blinkValue);

        // Mouth (vertical = aa, horizontal = ee)
        expressionManager.setValue('aa', mouthOpenVertical);
        expressionManager.setValue('ee', mouthOpenHorizontal);
      }
    };

    // Update VRM material emissive for bloom effect
    // Only update intensity, preserve the base color set during initialization
    window.updateVRMMaterial = function(intensity, color) {
      if (!currentVRM) {
        console.warn('[JS] updateVRMMaterial called but currentVRM is null');
        return;
      }

      const emissiveIntensity = Number(intensity) || 1.0;

      // Debug: log every 60 frames (assuming 60fps)
      let materialCount = 0;
      let updatedCount = 0;

      currentVRM.scene.traverse((node) => {
        if (node.isMesh && node.material) {
          const materials = Array.isArray(node.material) ? node.material : [node.material];
          materials.forEach((mat) => {
            materialCount++;
            if (mat.emissive && mat.emissiveIntensity !== undefined) {
              // Only update intensity, keep the color (which was copied from mat.color)
              mat.emissiveIntensity = emissiveIntensity;
              mat.needsUpdate = true;
              updatedCount++;
            }
          });
        }
      });

      if (frameCount % 60 === 0) {
        console.log('[DEBUG JS VRM] intensity=' + emissiveIntensity.toFixed(2) +
                    ' materials=' + materialCount + ' updated=' + updatedCount);
      }
    };

    // Update functions called from Ruby
    window.updateParticles = function(positions, colors, avgSize, avgOpacity) {
      if (!particleSystem) {
        console.warn('[JS] particleSystem is not initialized');
        return;
      }

      const posAttr = particleSystem.geometry.attributes.position;
      const colAttr = particleSystem.geometry.attributes.color;

      if (!posAttr || !colAttr) {
        console.warn('[JS] Attributes not found');
        return;
      }

      // Convert to numbers if needed
      const posArray = typeof positions === 'number' ? [positions] : Array.from(positions || []);
      const colArray = typeof colors === 'number' ? [colors] : Array.from(colors || []);

      if (posArray.length > 0 && colArray.length > 0) {
        for (let i = 0; i < posArray.length && i < posAttr.array.length; i++) {
          posAttr.array[i] = Number(posArray[i]) || 0;
        }
        for (let i = 0; i < colArray.length && i < colAttr.array.length; i++) {
          colAttr.array[i] = Number(colArray[i]) || 0;
        }

        posAttr.needsUpdate = true;
        colAttr.needsUpdate = true;

        // サイズと透明度の動的更新（Ruby で計算済みの平均値を使用）
        if (particleSystem.material) {
          particleSystem.material.size = Number(avgSize) || 0.05;
          particleSystem.material.opacity = Math.max(0.3, Math.min(1.0, Number(avgOpacity) || 0.8));
        }
      }
    };

    window.updateGeometry = function(scale, rotation, emissiveIntensity, color) {
      if (!geometryMesh) {
        console.warn('[JS] geometryMesh is not initialized');
        return;
      }

      const scaleNum = Number(scale) || 1.0;
      geometryMesh.scale.set(scaleNum, scaleNum, scaleNum);

      if (rotation && Array.isArray(rotation) && rotation.length >= 3) {
        geometryMesh.rotation.x = Number(rotation[0]) || 0;
        geometryMesh.rotation.y = Number(rotation[1]) || 0;
        geometryMesh.rotation.z = Number(rotation[2]) || 0;
      }

      // Ruby から渡された色を使用（新規追加）
      if (color && Array.isArray(color) && color.length >= 3 && geometryMesh.material) {
        const r = Number(color[0]) || 0;
        const g = Number(color[1]) || 0;
        const b = Number(color[2]) || 0;

        // Base color
        if (geometryMesh.material.color) {
          geometryMesh.material.color.setRGB(r, g, b);
        }

        // Emissive intensity (using emissiveIntensity property like VRM)
        if (emissiveIntensity !== undefined && geometryMesh.material.emissiveIntensity !== undefined) {
          const intensity = Number(emissiveIntensity) || 0;
          geometryMesh.material.emissiveIntensity = intensity;
          geometryMesh.material.needsUpdate = true;
        }
      }
    };

    window.updateBloom = function(strength, threshold) {
      if (!bloomPass) {
        console.warn('[JS] bloomPass is not initialized');
        return;
      }

      bloomPass.strength = strength !== undefined ? Number(strength) : 1.5;
      bloomPass.threshold = threshold !== undefined ? Number(threshold) : 0.0;

      // Debug: log every 120 frames
      if (frameCount % 120 === 0) {
        console.log('[DEBUG BLOOM] strength=' + bloomPass.strength.toFixed(2) +
                    ' threshold=' + bloomPass.threshold.toFixed(2));
      }
    };

    window.updateCamera = function(position, shake) {
      if (!camera) {
        console.warn('[JS] camera is not initialized');
        return;
      }

      const pos = Array.isArray(position) ? position : [0, 0, 5];
      const shk = Array.isArray(shake) ? shake : [0, 0, 0];

      camera.position.x = Number(pos[0]) + Number(shk[0]);
      camera.position.y = Number(pos[1]) + Number(shk[1]);
      camera.position.z = Number(pos[2]) + Number(shk[2]);
    };

    window.updateParticleRotation = function(rotation) {
      if (!particleSystem) {
        console.warn('[JS] particleSystem is not initialized');
        return;
      }

      if (rotation && Array.isArray(rotation) && rotation.length >= 3) {
        particleSystem.rotation.x = Number(rotation[0]) || 0;
        particleSystem.rotation.y = Number(rotation[1]) || 0;
        particleSystem.rotation.z = Number(rotation[2]) || 0;
      }
    };

    // Selective Bloom helper functions
    function darkenNonBloomed(obj) {
      if (obj.isMesh && bloomLayer.test(obj.layers) === false) {
        materials[obj.uuid] = obj.material;
        obj.material = darkMaterial;
      }
    }

    function restoreMaterial(obj) {
      if (materials[obj.uuid]) {
        obj.material = materials[obj.uuid];
        delete materials[obj.uuid];
      }
    }

    // Animation loop (thin: data extraction + Ruby callback + render)
    function animate() {
      requestAnimationFrame(animate);

      // Delta time for VRM
      const now = Date.now();
      const deltaTime = (now - animLastTime) / 1000;
      animLastTime = now;
      window._animDeltaTime = deltaTime;

      // Extract frequency data and call Ruby (Ruby handles FPS, debug, BPM)
      if (analyser && window.rubyUpdateVisuals) {
        try {
          analyser.getByteFrequencyData(dataArray);
          window.rubyUpdateVisuals(Array.from(dataArray), now);
        } catch (error) {
          console.error('[JS] Error calling Ruby update:', error);
        }
      }

      // Display update: read Ruby-formatted strings
      frameCount++;
      if (frameCount % 15 === 0) {  // Update DOM every ~15 frames
        if (window.fpsText) {
          document.getElementById('fpsCounter').textContent = window.fpsText;
        }
        if (window.debugInfoText) {
          document.getElementById('debugInfo').textContent = window.debugInfoText;
        }

        let paramText = window.paramInfoText || '';
        if (camera) {
          paramText += `  |  Cam: (${camera.position.x.toFixed(1)}, ${camera.position.y.toFixed(1)}, ${camera.position.z.toFixed(1)})`;
        }
        if (window.vrmDebugText) {
          paramText += `  |  ${window.vrmDebugText}`;
        }
        if (paramText) {
          document.getElementById('paramInfo').textContent = paramText;
        }
        if (window.keyGuideText) {
          document.getElementById('keyGuide').textContent = window.keyGuideText;
        }
      }

      // VRM spring bones update
      if (currentVRM) currentVRM.update(deltaTime);

      // Render
      if (composer) composer.render();
    }

    // Load Ruby code from embedded script tags
    async function loadRubyCode() {
      const rubyBlocks = document.querySelectorAll('script[type="text/ruby"]');
      for (const block of rubyBlocks) {
        const code = block.textContent;
        try {
          if (window.rubyVM) {
            window.rubyVM.eval(code);
            console.log(`[JS] Loaded Ruby code block: ${block.id}`);
          }
        } catch (error) {
          console.error(`Failed to load Ruby code block ${block.id}:`, error);
          throw error;
        }
      }
    }

    function updateLoadingStatus(message) {
      const el = document.getElementById('loadingStatus');
      if (el) {
        el.textContent = message;
      }
      console.log('[JS] ' + message);
    }

    // Initialize everything
    async function init() {
      try {
        // Always show VRM upload UI
        document.getElementById('vrmUpload').style.display = 'block';
        updateLoadingStatus('Upload a VRM file or start without it...');

        const vrmFile = await waitForVRMFile();
        document.getElementById('vrmUpload').style.display = 'none';

        updateLoadingStatus('Initializing Web Audio API...');
        await initAudio();

        updateLoadingStatus('Initializing Three.js...');
        initThree();

        // Load VRM if file was provided
        if (vrmFile) {
          updateLoadingStatus('Loading VRM model...');
          await loadVRMFile(vrmFile);
          // Adjust camera for VRM viewing
          camera.position.z = 1.5;
          console.log('[JS] Initialization complete with VRM!');
        } else {
          console.log('[JS] Initialization complete without VRM!');
        }

        updateLoadingStatus('Ready!');
        document.getElementById('loading').style.display = 'none';

        animate();
      } catch (error) {
        console.error('[JS] Initialization error:', error);
        updateLoadingStatus('Error: ' + error.message);
      }
    }

    // AudioContext の suspended 状態をユーザー操作で解除するフォールバック
    document.addEventListener('click', function resumeAudio() {
      if (audioContext && audioContext.state === 'suspended') {
        audioContext.resume().then(function() {
          console.log('[JS] AudioContext resumed by user click, state:', audioContext.state);
        });
      }
      document.removeEventListener('click', resumeAudio);
    });

    // VJ Pad prompt management
    const vjPrompt = document.getElementById('vjPrompt');
    const vjInput = document.getElementById('vjPromptInput');
    const vjResult = document.getElementById('vjPromptResult');
    const vjHistory = [];
    let vjHistoryIdx = -1;
    let vjPromptOpen = false;

    function vjPromptToggle() {
      vjPromptOpen = !vjPromptOpen;
      if (vjPromptOpen) {
        vjPrompt.classList.add('active');
        vjInput.value = '';
        vjResult.textContent = '';
        vjResult.className = '';
        vjHistoryIdx = -1;
        vjInput.focus();
      } else {
        vjPrompt.classList.remove('active');
        vjInput.blur();
      }
    }

    function vjPromptExec() {
      const cmd = vjInput.value.trim();
      if (!cmd) return;
      vjHistory.unshift(cmd);
      if (vjHistory.length > 50) vjHistory.pop();
      vjHistoryIdx = -1;
      let result = '';
      if (window.rubyExecPrompt) {
        try { result = window.rubyExecPrompt(cmd); } catch (e) { result = 'ERR: ' + e.message; }
      } else {
        result = 'ERR: Ruby VM not ready';
      }
      const isErr = typeof result === 'string' && result.startsWith('ERR:');
      vjResult.textContent = result || '';
      vjResult.className = isErr ? 'error' : '';
      vjInput.value = '';
    }

    // Prompt input key handling
    vjInput.addEventListener('keydown', function(e) {
      e.stopPropagation();
      if (e.key === 'Enter') { vjPromptExec(); }
      else if (e.key === 'Escape' || e.key === '`') { e.preventDefault(); vjPromptToggle(); }
      else if (e.key === 'ArrowUp') {
        e.preventDefault();
        if (vjHistory.length > 0 && vjHistoryIdx < vjHistory.length - 1) {
          vjHistoryIdx++;
          vjInput.value = vjHistory[vjHistoryIdx];
        }
      } else if (e.key === 'ArrowDown') {
        e.preventDefault();
        if (vjHistoryIdx > 0) { vjHistoryIdx--; vjInput.value = vjHistory[vjHistoryIdx]; }
        else { vjHistoryIdx = -1; vjInput.value = ''; }
      }
    });

    // Keyboard handler: Ruby handles config/color keys, JS handles camera (WebGL direct)
    window.addEventListener('keydown', function(event) {
      // When prompt is open, restore focus if lost, then let input handler process the key
      if (vjPromptOpen) {
        if (document.activeElement !== vjInput) {
          vjInput.focus();
        }
        return;
      }

      const key = event.key;

      // Backtick toggles VJ Pad prompt
      if (key === '`') { event.preventDefault(); vjPromptToggle(); return; }

      // Audio input controls (must stay in JS - direct Web Audio manipulation)
      if (key === 'm') { toggleMic(); return; }
      if (key === 't') { toggleTabCapture(); return; }

      // Camera position controls (must stay in JS - direct Three.js manipulation)
      if ('aswxqz'.includes(key) && camera) {
        const step = 0.5;
        if (key === 'a') camera.position.x -= step;
        else if (key === 's') camera.position.x += step;
        else if (key === 'w') camera.position.y += step;
        else if (key === 'x') camera.position.y -= step;
        else if (key === 'q') camera.position.z += step;
        else if (key === 'z') camera.position.z -= step;
        return;
      }

      // All other keys dispatched to Ruby master handler
      if (window.rubyHandleKey) {
        try {
          window.rubyHandleKey(key);
        } catch (error) {
          console.error('[JS] Error calling rubyHandleKey:', error);
        }
      }
    });

    console.log('[JS] Keyboard: Ruby handles config keys, JS handles camera (a/s/w/x/q/z, d/f/e/c), backtick(`) opens VJ Pad');

    // DevTool console interface for dynamic config changes
    // Usage: rubyConfig.set('sensitivity', 2.5) / .get('sensitivity') / .list() / .reset()
    window.rubyConfig = {
      set: (key, value) => window.rubyConfigSet?.(key, value) ?? 'Ruby VM not ready',
      get: (key) => window.rubyConfigGet?.(key) ?? 'Ruby VM not ready',
      list: () => window.rubyConfigList?.() ?? 'Ruby VM not ready',
      reset: () => window.rubyConfigReset?.() ?? 'Ruby VM not ready',
      help: () => console.log('Usage: rubyConfig.set(key, value) | .get(key) | .list() | .reset()')
    };

    // Handle window resize
    window.addEventListener('resize', function() {
      // Skip if Three.js not yet initialized
      if (!camera || !renderer || !composer) return;

      const width = window.innerWidth;
      const height = window.innerHeight;
      camera.aspect = width / height;
      camera.updateProjectionMatrix();
      renderer.setSize(width, height);
      composer.setSize(width, height);
    });

    // Start initialization when page loads
    window.addEventListener('DOMContentLoaded', init);
  </script>
</body>
</html>
